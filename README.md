# CS348-Applied-Optimization

This project compares two optimization techniques â€” **Gradient Descent** and **Newton's Method** â€” by applying them to a simple quadratic function and visualizing their convergence behavior.

## ðŸ“Œ Objective

- Minimize the function \( f(x) = x^2 + 4x + 4 \)
- Compare convergence rates of both methods
- Visualize optimization paths using `matplotlib`

## ðŸ“ˆ Function Details

- Function: `f(x) = x^2 + 4x + 4`
- Gradient: `f'(x) = 2x + 4`
- Hessian: `f''(x) = 2`

## ðŸš€ How to Run

1. Make sure you have Python 3 installed.
2. Install required packages (if not already installed):
   ```bash
    pip install numpy matplotlib
3. run the script:
   ```bash
  python gradient&newton.py

## ðŸ“Š Output
1. A plot comparing convergence paths of both methods
2. Newtonâ€™s method reaches the minimum faster




 
